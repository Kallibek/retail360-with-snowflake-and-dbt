import os
import logging
from datetime import datetime, timedelta
from airflow.utils.dates import days_ago
import pandas as pd
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
import random

DEFAULT_ARGS = {
    'owner': 'airflow',
    'start_date': days_ago(0),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

def _modify_s3_data(prefix, **context):
    """
    1) Find the latest CSV file in the given prefix (e.g. 'customers/', 'products/', 'stores/') in S3.
    2) Download and read into a pandas DataFrame.
    3) Modify ~5% of rows (and update created_at for only the modified rows).
    4) Re-upload only the modified rows as a CSV to S3 with a new filename.
    5) Log a sample of the changes (was vs. new).
    """
    bucket_name = os.environ.get('AWS_LANDING_BUCKET_NAME')
    if not bucket_name:
        raise ValueError("AWS_LANDING_BUCKET_NAME environment variable is not set.")
    
    s3_hook = S3Hook(aws_conn_id='aws_default')  # or your custom conn_id
    all_keys = s3_hook.list_keys(bucket_name=bucket_name, prefix=prefix)
    if not all_keys:
        logging.warning(f"No files found in S3 prefix: {prefix}")
        return
    
    # Sort and pick the newest file by name (assuming timestamp in filename or lexical order).
    all_keys_sorted = sorted(all_keys)
    latest_key = all_keys_sorted[-1]
    logging.info(f"Found {len(all_keys)} files under prefix '{prefix}'. Using the latest: {latest_key}")

    # Download the CSV to a temporary file
    downloaded_file = s3_hook.download_file(
        key=latest_key,
        bucket_name=bucket_name,
        local_path="/tmp",                # This must be a directory
        preserve_file_name=True,          # Default is True
        use_autogenerated_subdir=False,   # Avoid extra subfolders
    )

    # Load into pandas
    df = pd.read_csv(downloaded_file)

    # Determine how many rows to modify (~5%)
    total_rows = len(df)
    if total_rows == 0:
        logging.info(f"No rows in file {latest_key}, skipping modifications.")
        return

    num_to_modify = max(1, int(total_rows * 0.05))
    rows_to_modify = random.sample(range(total_rows), num_to_modify)

    # We'll store the changes to log them later
    changes = []
    now_str = datetime.utcnow().isoformat()

    # Decide on modifications based on prefix
    if prefix.startswith("customers"):
        # Modify last_name for chosen rows
        for row in rows_to_modify:
            old_value = df.at[row, 'last_name']
            new_value = "SNAPSHOT_TEST"
            df.at[row, 'last_name'] = new_value
            changes.append((row, old_value, new_value))
        # Update created_at only for modified rows
        df.loc[rows_to_modify, 'created_at'] = now_str

    elif prefix.startswith("products"):
        # Modify price for chosen rows (add $1 if not null, otherwise default to 10)
        for row in rows_to_modify:
            old_value = df.at[row, 'price']
            new_value = (old_value + 1) if pd.notnull(old_value) else 10
            df.at[row, 'price'] = new_value
            changes.append((row, old_value, new_value))
        # Update created_at only for modified rows
        df.loc[rows_to_modify, 'created_at'] = now_str

    elif prefix.startswith("stores"):
        # Modify store_name for chosen rows
        for row in rows_to_modify:
            old_value = df.at[row, 'store_name']
            new_value = f"{old_value}_SNAPSHOT"
            df.at[row, 'store_name'] = new_value
            changes.append((row, old_value, new_value))
        # Update created_at only for modified rows
        df.loc[rows_to_modify, 'created_at'] = now_str

    # Log a sample of changes (e.g., up to 5 entries)
    if changes:
        logging.info(f"Total changed rows: {len(changes)}. Showing up to 5 samples:")
        for idx, (row_idx, was, new) in enumerate(changes[:5], start=1):
            logging.info(f"  Sample {idx}: Row {row_idx} --> '{was}' changed to '{new}'")

    # Filter to only the modified rows
    modified_df = df.iloc[rows_to_modify].copy()

    # Write out the modified rows to new CSV
    timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
    new_key = f"{prefix.rstrip('/')}/{prefix.rstrip('/')}_{timestamp_str}_MODIFIED.csv"
    csv_data = modified_df.to_csv(index=False)

    # Upload the modified file
    logging.info(f"Uploading modified file to S3: {new_key}")
    s3_hook.load_string(
        string_data=csv_data,
        key=new_key,
        bucket_name=bucket_name,
        replace=True
    )
    logging.info("Modified file uploaded successfully.")

with DAG(
    dag_id='test_snapshot_modifications',
    default_args=DEFAULT_ARGS,
    description='Reads and modifies ~5% of existing customers/products/stores data, updates created_at for modified rows, and re-uploads only those modified rows to S3.',
    schedule_interval='@once',
    catchup=False
) as dag:

    modify_customers = PythonOperator(
        task_id='modify_customers',
        python_callable=_modify_s3_data,
        op_kwargs={'prefix': 'customers/'},
    )

    modify_products = PythonOperator(
        task_id='modify_products',
        python_callable=_modify_s3_data,
        op_kwargs={'prefix': 'products/'},
    )

    modify_stores = PythonOperator(
        task_id='modify_stores',
        python_callable=_modify_s3_data,
        op_kwargs={'prefix': 'stores/'},
    )

    [modify_customers, modify_products, modify_stores]
